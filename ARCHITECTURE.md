# ğŸ—ï¸ Architecture du systÃ¨me

Ce document dÃ©crit l'architecture technique de l'Assistant Fiction RAG.

## Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UTILISATEUR                               â”‚
â”‚                  (Ã‰crivain de fiction)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Ã‰dite/Consulte
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DOCUMENTS DE FICTION                            â”‚
â”‚   data/mon_projet/{lore,personnages,chapitres,...}           â”‚
â”‚              (Fichiers .md / .txt)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ 1. Indexation
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INDEXEUR                                   â”‚
â”‚              (src/indexer.py)                                â”‚
â”‚                                                              â”‚
â”‚  1. Charge les documents (loaders.py)                       â”‚
â”‚  2. DÃ©coupe en chunks (RecursiveCharacterTextSplitter)      â”‚
â”‚  3. CrÃ©e les embeddings (OpenAI Embeddings)                 â”‚
â”‚  4. Stocke dans ChromaDB                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Sauvegarde
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            BASE VECTORIELLE LOCALE                           â”‚
â”‚                  (ChromaDB)                                  â”‚
â”‚              db/mon_projet/                                  â”‚
â”‚                                                              â”‚
â”‚  Contient les embeddings + mÃ©tadonnÃ©es                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ 2. Utilisation
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INTERFACE CLI                                   â”‚
â”‚              (src/cli.py)                                    â”‚
â”‚                                                              â”‚
â”‚  - Chat interactif                                          â”‚
â”‚  - Commandes spÃ©ciales (/search, /sources)                 â”‚
â”‚  - Affichage formatÃ©                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Question utilisateur
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                MOTEUR RAG                                    â”‚
â”‚               (src/rag.py)                                   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  1. RETRIEVAL (RÃ©cupÃ©ration)                   â”‚        â”‚
â”‚  â”‚     - Vectorise la question                    â”‚        â”‚
â”‚  â”‚     - Recherche similaritÃ© dans ChromaDB       â”‚        â”‚
â”‚  â”‚     - RÃ©cupÃ¨re les k chunks pertinents         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â”‚                                          â”‚
â”‚                   â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  2. AUGMENTATION (Enrichissement)              â”‚        â”‚
â”‚  â”‚     - Construit le contexte avec chunks        â”‚        â”‚
â”‚  â”‚     - Ajoute le prompt spÃ©cialisÃ© fiction      â”‚        â”‚
â”‚  â”‚     - PrÃ©pare l'entrÃ©e pour le LLM             â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                   â”‚                                          â”‚
â”‚                   â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  3. GENERATION (CrÃ©ation)                      â”‚        â”‚
â”‚  â”‚     - Appel API OpenAI (GPT-4, etc.)           â”‚        â”‚
â”‚  â”‚     - GÃ©nÃ©ration de la rÃ©ponse                 â”‚        â”‚
â”‚  â”‚     - Retour au format appropriÃ©               â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ RÃ©ponse contextuelle
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UTILISATEUR                                â”‚
â”‚              ReÃ§oit une rÃ©ponse informÃ©e                     â”‚
â”‚           par son propre univers narratif                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Composants dÃ©taillÃ©s

### 1. Loaders (src/loaders.py)

**ResponsabilitÃ©** : Charger et prÃ©parer les documents

**Fonctions clÃ©s** :
- `load_project_documents()` : Parcourt rÃ©cursivement le dossier projet
- `split_documents()` : DÃ©coupe en chunks avec chevauchement

**Technologies** :
- LangChain TextLoader
- RecursiveCharacterTextSplitter

### 2. Indexer (src/indexer.py)

**ResponsabilitÃ©** : CrÃ©er et mettre Ã  jour l'index vectoriel

**Workflow** :
```
Documents â†’ Chunks â†’ Embeddings â†’ ChromaDB
```

**Technologies** :
- OpenAI Embeddings API (text-embedding-3-small)
- ChromaDB pour le stockage

### 3. RAG Engine (src/rag.py)

**ResponsabilitÃ©** : Orchestrer le processus RAG complet

**Composants** :

#### Retriever
```python
vectordb.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)
```
- Recherche de similaritÃ© cosine
- Retourne les k passages les plus pertinents

#### Prompt Template
```python
FICTION_PROMPT_TEMPLATE = """
Tu es un assistant crÃ©atif...
Context: {context}
Question: {question}
"""
```
- SpÃ©cialisÃ© pour l'Ã©criture de fiction
- Encourage la cohÃ©rence narrative

#### Chain
```python
RetrievalQA.from_chain_type(
    llm=ChatOpenAI,
    retriever=retriever,
    chain_type="stuff"
)
```
- Type "stuff" : met tout le contexte dans un seul prompt
- Alternative possible : "map_reduce" pour beaucoup de documents

### 4. CLI (src/cli.py)

**ResponsabilitÃ©** : Interface utilisateur

**FonctionnalitÃ©s** :
- Boucle interactive de chat
- Commandes spÃ©ciales (`/search`, `/sources`, `/help`)
- Affichage formatÃ© avec emojis et couleurs
- Gestion des erreurs

## Flux de donnÃ©es

### Flux d'indexation

```
1. Utilisateur modifie data/projet/chapitre1.md
2. Utilisateur lance: python -m src.indexer projet
3. Loaders charge tous les .md/.txt
4. Documents dÃ©coupÃ©s en chunks (1000 chars, overlap 150)
5. Chaque chunk â†’ embedding via OpenAI API
6. Embeddings + mÃ©tadonnÃ©es â†’ ChromaDB (db/projet/)
7. Index prÃªt Ã  l'utilisation
```

### Flux de requÃªte

```
1. Utilisateur pose question: "Qui est Alex ?"
2. CLI transmet Ã  RAG engine
3. Question â†’ embedding via OpenAI API
4. Recherche similaritÃ© dans ChromaDB
5. RÃ©cupÃ©ration des 5 chunks les plus proches
6. Construction du prompt avec contexte
7. Appel ChatGPT avec prompt enrichi
8. RÃ©ponse gÃ©nÃ©rÃ©e retournÃ©e Ã  CLI
9. Affichage formatÃ© Ã  l'utilisateur
```

## Structure des donnÃ©es

### Document Chunk

```python
{
    "page_content": "Texte du chunk...",
    "metadata": {
        "source": "data/projet/personnages/alex.md",
        "relative_path": "personnages/alex.md",
        "file_name": "alex.md"
    }
}
```

### Embedding Vector

- Dimension : 1536 (OpenAI text-embedding-3-small)
- Format : Liste de floats
- StockÃ© dans ChromaDB avec le chunk

### ChromaDB Collection

```python
{
    "name": "nom_projet",
    "embeddings": [...],  # Vecteurs
    "documents": [...],   # Textes originaux
    "metadatas": [...],   # MÃ©tadonnÃ©es
    "ids": [...]          # Identifiants uniques
}
```

## Configuration

### settings.yaml

```yaml
indexing:
  chunk_size: 1000       # Taille optimale pour narratif
  chunk_overlap: 150     # Ã‰vite coupures abruptes

rag:
  model: "gpt-4o-mini"   # Balance qualitÃ©/coÃ»t
  temperature: 0.7       # CrÃ©atif mais cohÃ©rent
  k_results: 5           # Contexte suffisant
```

### Variables d'environnement

```bash
OPENAI_API_KEY=sk-...   # Obligatoire
DEFAULT_MODEL=...       # Optionnel
DEFAULT_TEMPERATURE=... # Optionnel
```

## Optimisations possibles

### Performance

1. **Cache des embeddings** : Ne pas recrÃ©er si contenu inchangÃ©
2. **Indexation incrÃ©mentale** : Mettre Ã  jour seulement les fichiers modifiÃ©s
3. **Batch processing** : Traiter plusieurs chunks en parallÃ¨le

### QualitÃ©

1. **Hybrid search** : Combiner recherche vectorielle et BM25
2. **Reranking** : RÃ©ordonner les rÃ©sultats avec un modÃ¨le dÃ©diÃ©
3. **Metadata filtering** : Filtrer par type de document avant recherche

### ScalabilitÃ©

1. **ModÃ¨les locaux** : Ollama, LMStudio pour Ã©viter coÃ»ts API
2. **Base vectorielle distante** : Pinecone, Weaviate pour gros projets
3. **Chunking adaptatif** : Taille variable selon type de document

## SÃ©curitÃ© et confidentialitÃ©

### DonnÃ©es locales
- âœ… Documents stockÃ©s localement
- âœ… Base vectorielle locale (ChromaDB)

### DonnÃ©es externes
- âš ï¸ Embeddings crÃ©Ã©s via OpenAI API
- âš ï¸ Questions + contexte envoyÃ©s Ã  OpenAI
- âš ï¸ Pas de stockage par OpenAI (selon leurs conditions)

### Pour une confidentialitÃ© totale
- Utiliser Ollama ou LMStudio
- ModÃ¨les d'embeddings locaux (sentence-transformers)
- Tout reste sur votre machine

## DÃ©pendances

```
langchain          # Orchestration RAG
langchain-openai   # IntÃ©gration OpenAI
langchain-community # Loaders et utils
chromadb          # Base vectorielle
openai            # API client
tiktoken          # Tokenization
python-dotenv     # Config
pyyaml            # Settings
```

## ExtensibilitÃ©

### Ajouter un nouveau type de document

```python
# Dans loaders.py
from langchain_community.document_loaders import PDFLoader

if path.suffix == ".pdf":
    loader = PDFLoader(str(path))
```

### Changer de LLM

```python
# Dans rag.py
from langchain_community.llms import Ollama

llm = Ollama(model="llama2")
```

### Ajouter une interface web

```python
# Nouveau fichier: src/server.py
from fastapi import FastAPI
from src.rag import ask

app = FastAPI()

@app.post("/ask")
def query(project: str, question: str):
    return {"answer": ask(project, question)}
```

---

Cette architecture est conÃ§ue pour Ãªtre :
- **Simple** : Facile Ã  comprendre et modifier
- **Modulaire** : Chaque composant est indÃ©pendant
- **Extensible** : Facile d'ajouter des fonctionnalitÃ©s
- **Efficace** : Performant pour des projets de taille raisonnable


