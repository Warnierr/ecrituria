# Plan d'Upgrade Ecrituria v3.0

> ðŸŽ¯ **Objectif** : Transformer Ecrituria en assistant d'Ã©criture de classe mondiale pour fiction

## Contexte de la Recherche

AprÃ¨s une veille technologique approfondie (GitHub, HuggingFace, Medium, forums spÃ©cialisÃ©s), j'ai identifiÃ© **15 axes d'amÃ©lioration majeurs** inspirÃ©s des meilleures pratiques 2024 en RAG pour creative writing.

---

## ðŸ“Š RÃ©sultats de la Recherche

### Projets Similaires AnalysÃ©s

| Projet | Points Forts | Ã€ Retenir |
|--------|--------------|-----------|
| **Sudowrite** | Story Bible, Muse model optimisÃ© fiction | Feedback narratif, gÃ©nÃ©ration beat-by-beat |
| **NovelCrafter** | Codex illimitÃ©, BYOM, Tinker Chat contextuel | FlexibilitÃ© AI models, wiki intÃ©grÃ© |
| **Novel-OS** | Workflow structurÃ© pour contexte | Organisation mÃ©thodique |
| **RAGFlow** | Deep document understanding (PDFs) | Extraction haute fidÃ©litÃ© |

### Frameworks RAG 2024

| Framework | Performance | Use Case IdÃ©al |
|-----------|-------------|----------------|
| **LlamaIndex** | âš¡ 2-5x plus rapide retrieval | Knowledge bases, docs massifs |
| **LangChain** | ðŸ”§ ModularitÃ© maximale | Orchestration complexe, agents |
| **Haystack** | ðŸ­ Production-ready | Pipelines enterprise |
| **Dify** | ðŸŽ¨ Visual workflow | No-code development |

### Embedding Models (HuggingFace MTEB 2024)

1. **BGE-M3** : Multilingual, long context (jusqu'Ã  8192 tokens)
2. **E5-Mistral-7B** : SOTA pour semantic search
3. **All-MPNet-base-v2** : Balance performance/vitesse
4. **Cohere Embed v3** : Robustesse au bruit

---

## ðŸš€ Plan d'AmÃ©lioration en 3 Phases

## Phase 1 : Optimisations RAG Core (ðŸŸ¢ PrioritÃ© Haute)

> **Impact** : +40% prÃ©cision retrieval, -50% hallucinations

### 1.1 Retrieval Hybride BM25 + Vectoriel

**ProblÃ¨me actuel** : ChromaDB seul peut manquer des correspondances exactes

**Solution** :
```python
# Combiner recherche lexicale (BM25) + sÃ©mantique
from rank_bm25 import BM25Okapi
from llama_index.core import VectorStoreIndex

class HybridRetriever:
    def retrieve(self, query, k=5):
        bm25_results = self.bm25.get_top_n(query, k*2)
        vector_results = self.vector_index.as_retriever(k=k*2)
        return self.rerank(bm25_results + vector_results, k)
```

**Gain** : +20-30% recall selon benchmarks

---

### 1.2 Upgrade Embedding Model

**Actuel** : OpenAI embeddings (propriÃ©taires, coÃ»t, latence)

**Proposition** : **BGE-M3** (HuggingFace)
- Open source, gratuit
- 8192 tokens context (vs 8191 OpenAI)
- Top MTEB leaderboard 2024
- Multilingual (utile pour Å“uvres traduites)

**ImplÃ©mentation** :
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-m3')
embeddings = model.encode(texts, normalize_embeddings=True)
```

**Ã‰conomie** : ~$100-500/mois si volume Ã©levÃ©

---

### 1.3 Chunking SÃ©mantique AvancÃ©

**Actuel** : Chunks fixes de 300-500 tokens

**Meilleures Pratiques 2024** :
- **Semantic chunking** : DÃ©couper aux limites naturelles (scÃ¨nes, paragraphes, dialogues)
- **Sliding window** avec overlap intelligent
- **Metadata enrichment** : Tags auto (personnage, lieu, temps, Ã©motion)

**Exemple** :
```python
from llama_index.core.node_parser import SemanticSplitterNodeParser

splitter = SemanticSplitterNodeParser(
    breakpoint_percentile_threshold=95,  # Seuil sÃ©mantique
    embed_model=embed_model
)
```

---

### 1.4 Reranking Post-Retrieval

**Principe** : RÃ©ordonner les chunks rÃ©cupÃ©rÃ©s par pertinence rÃ©elle

**ModÃ¨les suggÃ©rÃ©s** :
- **MS MARCO Cross-Encoder** (gratuit, HuggingFace)
- **Cohere Rerank** (API, trÃ¨s prÃ©cis)

**Impact** : +15-25% prÃ©cision finale

---

## Phase 2 : Architecture Moderne (ðŸŸ¡ PrioritÃ© Moyenne)

> **Impact** : FlexibilitÃ©, maintenabilitÃ©, extensibilitÃ©

### 2.1 Migration vers LlamaIndex

**Pourquoi ?**
- âœ… 2-5x plus rapide pour retrieval massif
- âœ… Support natif GraphRAG
- âœ… Ã‰valuation RAG intÃ©grÃ©e (fidÃ©litÃ©, pertinence)
- âœ… Meilleure gestion multi-documents

**Migration progressive** :
1. Wrapper LlamaIndex autour de ChromaDB existant
2. Remplacer progressivement les loaders
3. Adopter `QueryEngine` pour orchestration

**Code** :
```python
from llama_index.core import VectorStoreIndex, ServiceContext
from llama_index.vector_stores.chroma import ChromaVectorStore

vector_store = ChromaVectorStore(chroma_collection=collection)
index = VectorStoreIndex.from_vector_store(vector_store)
query_engine = index.as_query_engine(similarity_top_k=5)
```

---

### 2.2 GraphRAG avec Neo4j

**Concept** : Enrichir le RAG vectoriel avec un graphe de connaissances

**BÃ©nÃ©fices pour fiction** :
- Traquer relations complexes (personnages, lieux, Ã©vÃ©nements)
- "Qui connaÃ®t qui ?" "Quel Ã©vÃ©nement a causÃ© quoi ?"
- DÃ©tection incohÃ©rences narratives

**Architecture** :
```
[Documents] â†’ LLM Extraction â†’ [Graphe Neo4j]
                                      â†“
[RequÃªte] â†’ Retrieval Hybride â†’ Vecteurs + Sous-graphe
                                      â†“
                           Contexte enrichi â†’ LLM
```

**ImplÃ©mentation** :
```python
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.core import KnowledgeGraphIndex

graph_store = Neo4jGraphStore(url="bolt://localhost:7687")
kg_index = KnowledgeGraphIndex.from_documents(
    documents, 
    graph_store=graph_store,
    max_triplets_per_chunk=10
)
```

**Exemple requÃªte** :
> "Quelles sont toutes les scÃ¨nes oÃ¹ Alex Chen et le Baron des Cendres interagissent indirectement ?"

---

### 2.3 BYOM (Bring Your Own Models)

**Inspiration** : NovelCrafter

**Permettre de connecter** :
- OpenRouter (300+ modÃ¨les)
- Anthropic Claude Sonnet/Opus
- Google Gemini Pro
- Llama 3.3 70B (local ou groq)
- Mistral Large

**Interface** :
```python
# src/llm_providers.py - Extension
PROVIDERS = {
    "openrouter": OpenRouterLLM,
    "anthropic": AnthropicLLM,
    "google": GoogleLLM,
    "ollama": OllamaLLM  # Local !
}

# .env
LLM_PROVIDER=anthropic
LLM_MODEL=claude-3-7-sonnet-20250219
LLM_API_KEY=sk-...
```

---

### 2.4 Frontend React/Next.js

**Actuel** : HTML/JS/CSS vanilla dans FastAPI

**Proposition** : Stack moderne dÃ©couplÃ©e

**Avantages** :
- Hot reload dÃ©veloppement
- Composants rÃ©utilisables
- State management (Zustand/Redux)
- DÃ©ploiement indÃ©pendant (Vercel/Netlify)

**Stack suggÃ©rÃ©e** :
```
Frontend : Next.js 15 + TypeScript + TailwindCSS + shadcn/ui
Backend  : FastAPI (API seulement)
Websocket: Live updates indexation/gÃ©nÃ©ration
```

**Structure** :
```
ecrituria/
â”œâ”€â”€ frontend/          # Next.js app
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ FileTree.tsx
â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx
â”‚   â”‚   â”œâ”€â”€ UploadModal.tsx
â”‚   â”‚   â””â”€â”€ KnowledgeGraph.tsx  # Viz Neo4j !
â”‚   â””â”€â”€ pages/
â””â”€â”€ backend/           # FastAPI
    â””â”€â”€ src/
```

---

## Phase 3 : FonctionnalitÃ©s CrÃ©atives (ðŸ”µ PrioritÃ© Basse)

> **Impact** : ExpÃ©rience utilisateur, diffÃ©renciation

### 3.1 Story Bible Contextuel

**Inspiration** : Sudowrite + NovelCrafter

**FonctionnalitÃ©s** :
- **Auto-extraction** : Personnages, lieux, objets mentionnÃ©s
- **Timeline visuelle** : Chronologie Ã©vÃ©nements
- **Character arcs** : Ã‰volution personnages
- **Consistency checker** : DÃ©tection contradictions

**UI** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Codex (Sidebar)â”‚ Main Editor  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
â”‚ ðŸ‘¤ Personnages  â”‚  Chapitre 5  â”‚
â”‚   â€¢ Alex Chen   â”‚              â”‚
â”‚   â€¢ Elara Voss  â”‚  [contenu]   â”‚
â”‚ ðŸ“ Lieux        â”‚              â”‚
â”‚ â±ï¸ Timeline     â”‚              â”‚
â”‚ ðŸŽ­ Intrigues    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.2 Tinker Chat Contextuel

**Concept** : Chat IA qui connaÃ®t TOUT le contexte du projet

**CapacitÃ©s** :
- "GÃ©nÃ¨re-moi un dialogue entre Alex et Elara"
- "RÃ©sume l'intrigue du chapitre 3"
- "Trouve les incohÃ©rences de timeline"
- "SuggÃ¨re des idÃ©es pour la fin"

**Backend** :
```python
@app.post("/api/tinker-chat")
async def tinker_chat(query: str, project: str):
    # RÃ©cupÃ¨re contexte complet
    codex = load_codex(project)
    timeline = get_timeline(project)
    current_chapter = get_active_file()
    
    # RAG enrichi
    context = hybrid_retrieve(query, k=10)
    
    # Prompt structurÃ©
    prompt = f"""Context:
    - Codex: {codex}
    - Timeline: {timeline}
    - Current: {current_chapter}
    - Retrieved: {context}
    
    User: {query}
    Assistant:"""
```

---

### 3.3 Analyse Narrative

**MÃ©triques** :
- Pacing (rythme par chapitre)
- Sentiment analysis (arcs Ã©motionnels)
- Dialogue/Narration ratio
- ComplexitÃ© lexicale
- POV consistency

**Viz** : Graphiques interactifs (Chart.js/Recharts)

---

### 3.4 Export Multi-format

**Formats** :
- âœ… Markdown (actuel)
- âž• DOCX (manuscrit Ã©diteurs)
- âž• EPUB (ebook)
- âž• PDF (formatÃ©)
- âž• Scrivener (.scriv)

**Lib** : `python-docx`, `ebooklib`, `pandoc`

---

## ðŸ› ï¸ Recommandations Stack Finale

### Option A : Ã‰volution Progressive (RecommandÃ©)

**Keep** :
- FastAPI
- ChromaDB
- Python backend

**Add** :
- LlamaIndex (wrapper)
- BGE-M3 embeddings
- Hybrid retrieval (BM25 + Vector)
- Reranking
- Neo4j (GraphRAG)

**Migrate** :
- Frontend â†’ Next.js (optionnel, impact fort)

**CoÃ»t** : Faible, mostly OSS

---

### Option B : Refonte Totale

**Stack** :
- **Backend** : Node.js + LangChain.js OU FastAPI + LlamaIndex
- **Frontend** : Next.js 15 + TypeScript
- **Vector DB** : Qdrant (plus performant que Chroma)
- **Graph DB** : Neo4j
- **Embeddings** : BGE-M3 (HuggingFace)
- **LLM** : BYOM (OpenRouter, Anthropic, Google)
- **Deploy** : Docker Compose

**CoÃ»t** : Moyen (temps dev)
**ROI** : TrÃ¨s haute maintenabilitÃ©

---

## ðŸ“ˆ Roadmap ImplÃ©mentation

### Sprint 1 (2 semaines) : RAG Core
- [ ] ImplÃ©menter BM25 retrieval
- [ ] IntÃ©grer BGE-M3 embeddings
- [ ] Ajouter reranking

### Sprint 2 (2 semaines) : GraphRAG
- [ ] Setup Neo4j
- [ ] Extraction entitÃ©s/relations LLM
- [ ] Query engine hybride

### Sprint 3 (3 semaines) : Frontend Moderne
- [ ] Init Next.js app
- [ ] Migration composants
- [ ] Graph visualization

### Sprint 4 (1 semaine) : FonctionnalitÃ©s CrÃ©atives
- [ ] Story Bible auto
- [ ] Tinker Chat
- [ ] Analytics

---

## ðŸ’¡ Innovations Uniques Ã  ConsidÃ©rer

### 1. **Voice-to-Text Integration**
Dicter des idÃ©es directement (Whisper API)

### 2. **Collaborative Mode**
Ã‰criture Ã  plusieurs (WebSocket, Y.js CRDT)

### 3. **AI Critique Mode**
Feedback de type beta-reader (structure, pacing, cohÃ©rence)

### 4. **Version Control Narratif**
Git-like pour branches narratives alternatives

### 5. **Research Assistant**
Recherche web contextuelle pour documentation

---

## âš ï¸ Points d'Attention

> [!IMPORTANT]
> **Credits OpenRouter** : VÃ©rifier solde, configurer fallback models

> [!WARNING]
> **Migration ChromaDB** : Backup complet avant upgrade embeddings

> [!CAUTION]
> **GraphRAG Complexity** : Commencer simple, itÃ©rer progressivement

---

## ðŸ“š Ressources ComplÃ©mentaires

### Documentation
- [LlamaIndex Docs](https://docs.llamaindex.ai/)
- [Microsoft GraphRAG](https://microsoft.github.io/graphrag/)
- [HuggingFace MTEB](https://huggingface.co/spaces/mteb/leaderboard)

### Repos GitHub Inspirants
- `forsonny/book-os` - Novel workflow
- `yanis112/LocalRAG` - RAG local complet
- `microsoft/graphrag` - GraphRAG officiel

### Articles ClÃ©s
- "RAG Best Practices for Fiction" (Medium)
- "LlamaIndex vs LangChain 2024" (TowardsAI)
- "Hybrid Search Deep Dive" (Weaviate Blog)

---

## ðŸŽ¯ MÃ©triques de SuccÃ¨s

| MÃ©trique | Avant | Cible v3.0 |
|----------|-------|------------|
| Retrieval Precision | ~65% | **90%+** |
| Response Latency | 3-5s | **<2s** |
| Hallucination Rate | ~15% | **<5%** |
| User Satisfaction | ? | **8/10+** |
| Cost per Query | $0.05 | **$0.01** |

---

**Prochaine Ã©tape** : Valider les prioritÃ©s avec vous et commencer Sprint 1 ! ðŸš€
